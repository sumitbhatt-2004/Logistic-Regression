{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Question:"
      ],
      "metadata": {
        "id": "Yz645cgUYzLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1) What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Solution :\n",
        "  \n",
        "   **Logistic Regression**: A classification algorithm used to predict the probability of a binary outcome. It models the relationship between a set of independent variables and a dependent variable that can take only two values (0 or 1, yes or no). The core of Logistic Regression is the *sigmoid function*, which transforms the output of a linear equation into a probability between 0 and 1.\n",
        "\n",
        "  **Linear Regression**: A regression algorithm used to predict a continuous dependent variable based on one or more independent variables. It models a linear relationship between the variables, attempting to find the best-fitting straight line (or hyperplane in higher dimensions) through the data.\n",
        "                    \n",
        "\n",
        "Question 2) What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Solution :\n",
        "\n",
        "    The Logistic Regression equation consists of two main parts:\n",
        "\n",
        "    1.  **Linear Combination**:\n",
        "        `z = b0 + b1*x1 + b2*x2 + ... + bn*xn`\n",
        "\n",
        "        *   Where:\n",
        "            *   `z` is the linear combination of input features.\n",
        "            *   `b0` is the y-intercept (bias).\n",
        "            *   `b1, b2, ..., bn` are the coefficients (weights) of the features.\n",
        "            *   `x1, x2, ..., xn` are the input features.\n",
        "    2.  **Sigmoid Function**:\n",
        "        `p = 1 / (1 + e^(-z))`\n",
        "\n",
        "        *   Where:\n",
        "            *   `p` is the predicted probability of the positive class (0 or 1).\n",
        "            *   `e` is the base of the natural logarithm (Euler's number, approximately 2.71828).\n",
        "            *   `z` is the result of the linear combination from step 1.\n",
        "    *   The sigmoid function ensures that the output `p` is always between 0 and 1, making it interpretable as a probability.\n",
        "\n",
        "Question : 3) Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "solution :\n",
        "\n",
        "    *   The *Sigmoid function* (also known as the logistic function) is used in Logistic Regression for several key reasons:\n",
        "        1.  **Probability Output**: It maps any real-valued number into a value between 0 and 1, which can be interpreted as the probability of belonging to a particular class.\n",
        "        2.  **Non-Linearity**: It introduces non-linearity into the model, allowing Logistic Regression to model complex relationships between the independent variables and the probability of the dependent variable.\n",
        "        3.  **Differentiability**: It is differentiable, which is essential for gradient-based optimization algorithms used to train the model.\n",
        "        4.  **Easy Interpretation**: The output of the sigmoid function can be easily interpreted as the probability of the positive class. If `p > 0.5`, the instance is predicted as the positive class; otherwise, it is predicted as the negative class.\n",
        "\n",
        "Question  4) What is the cost function of Logistic Regression?\n",
        "\n",
        "solution :  The cost function in Logistic Regression, also known as the *Log Loss* or *Binary Cross-Entropy Loss*, quantifies the error between predicted probabilities and the actual labels.\n",
        "    *   For a single data point, the Log Loss is defined as:\n",
        "\n",
        "        `Cost(p, y) = -y * log(p) - (1 - y) * log(1 - p)`\n",
        "\n",
        "        *   Where:\n",
        "\n",
        "            *   `p` is the predicted probability of the positive class.\n",
        "            *   `y` is the actual label (0 or 1).\n",
        "        *   When `y = 1` (positive class), the cost is `-log(p)`. So, if `p` is close to 1 (correct prediction), the cost is close to 0. If `p` is close to 0 (incorrect prediction), the cost approaches infinity.\n",
        "        *   When `y = 0` (negative class), the cost is `-log(1 - p)`. So, if `p` is close to 0 (correct prediction), the cost is close to 0. If `p` is close to 1 (incorrect prediction), the cost approaches infinity.\n",
        "    *   The overall cost function for the entire dataset is the average of the Log Loss over all data points:\n",
        "\n",
        "        `J = (1/n) * Σ [ -yi * log(pi) - (1 - yi) * log(1 - pi) ]`\n",
        "\n",
        "        *   Where:\n",
        "\n",
        "            *   `n` is the number of data points.\n",
        "            *   `yi` is the actual label for the i-th data point.\n",
        "            *   `pi` is the predicted probability for the i-th data point.\n",
        "    *   The goal of Logistic Regression is to minimize this cost function, finding the parameters that best fit the data.\n",
        "\n",
        "Question  5) What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "solution : *Regularization* is a technique used to prevent overfitting in Logistic Regression (and other machine learning models) by adding a penalty term to the cost function. This penalty discourages the model from assigning overly large coefficients to the features.\n",
        "\n",
        "    *   **Why is it needed?**\n",
        "        *   **Overfitting**: Overfitting occurs when a model learns the training data too well, capturing noise and outliers. This results in excellent performance on the training set but poor generalization to new, unseen data.\n",
        "        *   **High Variance**: Overfitted models have high variance, meaning they are very sensitive to small fluctuations in the training data.\n",
        "        *   **Complexity**: Overfitting often leads to complex models with large coefficients, which can be difficult to interpret and may not generalize well.\n",
        "    *   **How Regularization Works**:\n",
        "        *   Regularization adds a penalty term to the cost function that is proportional to the magnitude of the coefficients. The goal is to minimize the cost function while keeping the coefficients small.\n",
        "        *   The two most common types of regularization in Logistic Regression are L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "Question 6) Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "solution :\n",
        "\n",
        "    *   *Lasso (L1) Regression*:\n",
        "\n",
        "        *   Adds a penalty term proportional to the *absolute value* of the coefficients to the cost function.\n",
        "        *   `Cost Function = Original Cost + λ * Σ |βi|`\n",
        "        *   Where:\n",
        "            *   `λ` is the regularization parameter.\n",
        "            *   `βi` are the coefficients of the features.\n",
        "        *   **Feature Selection**: Lasso can shrink some coefficients to exactly zero, effectively performing feature selection. This makes it useful when dealing with datasets with many irrelevant or redundant features.\n",
        "    *   *Ridge (L2) Regression*:\n",
        "\n",
        "        *   Adds a penalty term proportional to the *square* of the coefficients to the cost function.\n",
        "        *   `Cost Function = Original Cost + λ * Σ βi^2`\n",
        "        *   Where:\n",
        "            *   `λ` is the regularization parameter.\n",
        "            *   `βi` are the coefficients of the features.\n",
        "        *   **Coefficient Reduction**: Ridge reduces the magnitude of all coefficients but does not force any to zero. It is effective in reducing multicollinearity and improving the stability of the model.\n",
        "    *   *Elastic Net Regression*:\n",
        "\n",
        "        *   Combines L1 (Lasso) and L2 (Ridge) regularization.\n",
        "        *   `Cost Function = Original Cost + λ1 * Σ |βi| + λ2 * Σ βi^2`\n",
        "        *   Where:\n",
        "            *   `λ1` and `λ2` are the regularization parameters for L1 and L2 penalties, respectively.\n",
        "        *   **Hybrid Approach**: Elastic Net balances feature selection and coefficient reduction. It is useful when dealing with datasets with many features, some of which may be correlated.\n",
        "    *   **Summary Table**:\n",
        "\n",
        "        | Feature          | Lasso (L1)                          | Ridge (L2)                          | Elastic Net                              |\n",
        "        | :--------------- | :---------------------------------- | :---------------------------------- | :--------------------------------------- |\n",
        "        | **Penalty**      | Absolute value of coefficients      | Square of coefficients              | Combination of L1 and L2                 |\n",
        "        | **Effect**       | Feature selection (zeros out coeffs) | Reduces coefficient magnitude       | Balances feature selection and reduction |\n",
        "        | **Use Case**     | Many irrelevant features            | Multicollinearity                   | High-dimensional data with correlations |\n",
        "\n",
        "Question  7) When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "solution :\n",
        "\n",
        "    *   Elastic Net is most appropriate in scenarios where:\n",
        "        *   **High-Dimensional Data**: When you have a large number of features compared to the number of observations.\n",
        "        *   **Multicollinearity**: When there are correlations among the features.\n",
        "        *   **Uncertainty**: When you are unsure whether Lasso or Ridge is more appropriate. Elastic Net provides a balance between feature selection and coefficient reduction.\n",
        "        *   **Lasso Limitations**: When Lasso selects only one feature from a group of highly correlated features, Elastic Net can select multiple correlated features, which may be more appropriate.\n",
        "    *   In summary, Elastic Net is a good choice when you need both feature selection and coefficient reduction, especially when dealing with complex datasets with many features and potential correlations.\n",
        "\n",
        "Question  8) What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "solution :  The regularization parameter, often denoted as `λ` (lambda), controls the strength of the regularization penalty. It determines the trade-off between fitting the training data well and keeping the coefficients small.\n",
        "\n",
        "        *   **Large λ (Strong Regularization)**:\n",
        "\n",
        "            *   **Effect**: The penalty term becomes more significant, forcing the model to reduce the magnitude of the coefficients.\n",
        "            *   **Impact**:\n",
        "                *   **Simpler Model**: The model becomes simpler, with smaller coefficients.\n",
        "                *   **Reduced Overfitting**: Helps prevent overfitting by reducing the model's sensitivity to noise in the training data.\n",
        "                *   **Increased Bias**: Can lead to underfitting if `λ` is too large, as the model may become too simple to capture the underlying patterns in the data.\n",
        "                *   **Coefficient Shrinkage**: Coefficients are pushed towards zero. In Lasso (L1) regularization, some coefficients may be exactly zero, leading to feature selection.\n",
        "        *   **Small λ (Weak Regularization)**:\n",
        "\n",
        "            *   **Effect**: The penalty term becomes less significant, allowing the model to fit the training data more closely.\n",
        "            *   **Impact**:\n",
        "                *   **Complex Model**: The model becomes more complex, with larger coefficients.\n",
        "                *   **Increased Overfitting**: Can lead to overfitting, especially if the model is too complex for the amount of training data available.\n",
        "                *   **Reduced Bias**: The model can capture more of the underlying patterns in the data.\n",
        "        *   **λ = 0 (No Regularization)**:\n",
        "\n",
        "            *   **Effect**: There is no regularization penalty.\n",
        "            *   **Impact**:\n",
        "                *   **Unconstrained Model**: The model can fit the training data as closely as possible.\n",
        "                *   **Maximum Overfitting**: High risk of overfitting, especially with complex models and limited data.\n",
        "    *   **Choosing the Right λ**:\n",
        "        *   The optimal value of `λ` is typically determined using techniques such as cross-validation. You can test a range of `λ` values and select the one that provides the best performance on a validation set.\n",
        "\n",
        "Question  9) What are the key assumptions of Logistic Regression?\n",
        "\n",
        "solution :    Logistic Regression makes several key assumptions about the data:\n",
        "\n",
        "        1.  **Linearity of Log-Odds**: Logistic Regression assumes a linear relationship between the independent variables and the *log-odds* (logit) of the dependent variable. The log-odds is defined as `log(p / (1 - p))`, where `p` is the probability of the positive class.\n",
        "        2.  **Independence of Errors**: The errors (residuals) are assumed to be independent of each other. This means that the error for one data point should not be correlated with the error for another data point.\n",
        "        3.  **Absence of Multicollinearity**: Multicollinearity occurs when independent variables are highly correlated with each other. Logistic Regression assumes that there is little or no multicollinearity among the independent variables. High multicollinearity can lead to unstable and unreliable coefficient estimates.\n",
        "        4.  **Sufficiently Large Dataset**: Logistic Regression requires a sufficiently large dataset to provide stable and reliable estimates of the coefficients. A general rule of thumb is to have at least 10 events (positive outcomes) per independent variable.\n",
        "        5.  **Correct Specification**: The model should include all relevant variables and exclude irrelevant ones. Omission of important variables can lead to biased coefficient estimates, while inclusion of irrelevant variables can reduce the precision of the estimates.\n",
        "\n",
        "Question 10) What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "solution :  While Logistic Regression is a powerful and widely used classification algorithm, there are several alternatives that may be more appropriate depending on the specific characteristics of the data and the problem at hand. Some common alternatives include:\n",
        "        1.  **Support Vector Machines (SVM)**: Effective in high-dimensional spaces and can model non-linear relationships using kernel functions.\n",
        "        2.  **Decision Trees**: Simple to understand and interpret, can handle both categorical and numerical data, and can model non-linear relationships.\n",
        "        3.  **Random Forests**: Ensemble learning method that combines multiple decision trees to improve accuracy and reduce overfitting.\n",
        "        4.  **Gradient Boosting Machines (GBM)**: Another ensemble learning method that builds a strong classifier by combining weak learners (typically decision trees) in a sequential manner.\n",
        "        5.  **Neural Networks**: Can model complex non-linear relationships and are suitable for large datasets with many features.\n",
        "        6.  **Naive Bayes**: Based on Bayes' theorem and assumes independence between features. It is computationally efficient and works well with high-dimensional data.\n",
        "        7.  **K-Nearest Neighbors (KNN)**: Classifies data points based on the majority class of their k-nearest neighbors in the feature space.\n",
        "\n",
        "Question 11) What are Classification Evaluation Metrics?\n",
        "\n",
        "solution :\n",
        "\n",
        "    *   *Classification evaluation metrics* are used to assess the performance of a classification model. These metrics provide insights into how well the model is classifying instances into different categories. Common classification evaluation metrics include:\n",
        "        1.  **Accuracy**:\n",
        "            *   *Definition*: The proportion of correctly classified instances out of the total number of instances.\n",
        "            *   `Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
        "            *   Where:\n",
        "                *   `TP` = True Positives\n",
        "                *   `TN` = True Negatives\n",
        "                *   `FP` = False Positives\n",
        "                *   `FN` = False Negatives\n",
        "            *   *Use Case*: Useful when the classes are balanced.\n",
        "        2.  **Precision**:\n",
        "            *   *Definition*: The proportion of true positive predictions out of all positive predictions.\n",
        "            *   `Precision = TP / (TP + FP)`\n",
        "            *   *Use Case*: Measures how well the model avoids false positives.\n",
        "        3.  **Recall (Sensitivity or True Positive Rate)**:\n",
        "            *   *Definition*: The proportion of true positive predictions out of all actual positive instances.\n",
        "            *   `Recall = TP / (TP + FN)`\n",
        "            *   *Use Case*: Measures how well the model avoids false negatives.\n",
        "        4.  **F1-Score**:\n",
        "            *   *Definition*: The harmonic mean of precision and recall.\n",
        "            *   `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
        "            *   *Use Case*: Provides a balanced measure of precision and recall, useful when classes are imbalanced.\n",
        "        5.  **AUC-ROC (Area Under the Receiver Operating Characteristic Curve)**:\n",
        "            *   *Definition*: Measures the ability of the model to distinguish between classes at various threshold settings.\n",
        "            *   *Use Case*: Useful when you want to evaluate the model's performance across different probability thresholds.\n",
        "            *   **ROC Curve**: Plots the true positive rate (recall) against the false positive rate at various threshold settings.\n",
        "            *   **AUC**: Represents the area under the ROC curve. A higher AUC indicates better performance.\n",
        "        6.  **Confusion Matrix**:\n",
        "            *   *Definition*: A table that summarizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
        "            *   *Use Case*: Provides a detailed breakdown of the model's performance, allowing you to identify specific areas where the model excels or struggles.\n",
        "\n",
        "Question  12) How does class imbalance affect Logistic Regression?\n",
        "\n",
        "solution :\n",
        "   *Class imbalance* occurs when the number of instances in one class is significantly higher than the number of instances in another class. This can have a significant impact on the performance of Logistic Regression:\n",
        "        1.  **Biased Predictions**: Logistic Regression tends to be biased towards the majority class, as it tries to minimize the overall error rate. This can lead to poor performance on the minority class, which is often the class of interest.\n",
        "        2.  **Misleading Accuracy**: Accuracy can be misleading when there is a class imbalance. A model that always predicts the majority class can achieve high accuracy, even if it performs poorly on the minority class.\n",
        "        3.  **Poor Generalization**: Models trained on imbalanced datasets may not generalize well to new, unseen data, especially for the minority class.\n",
        "    *   **Techniques for Handling Class Imbalance**:\n",
        "        1.  **Resampling Techniques**:\n",
        "            *   **Oversampling**: Increase the number of instances in the minority class by duplicating existing instances or creating synthetic instances (e.g., using SMOTE).\n",
        "            *   **Undersampling**: Decrease the number of instances in the majority class by randomly removing instances.\n",
        "        2.  **Cost-Sensitive Learning**: Assign different misclassification costs to different classes, giving higher weight to misclassifications of the minority class.\n",
        "        3.  **Threshold Adjustment**: Adjust the classification threshold to optimize performance on the minority class. The default threshold is typically 0.5, but you can adjust it to a different value based on the specific requirements of the problem.\n",
        "        4.  **Ensemble Methods**: Use ensemble methods like Random Forests or Gradient Boosting, which can handle class imbalance more effectively than Logistic Regression.\n",
        "        5.  **Evaluation Metrics**: Use appropriate evaluation metrics that are less sensitive to class imbalance, such as precision, recall, F1-score, and AUC-ROC.\n",
        "\n",
        "Question :  13) What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Solution: Hyperparameter tuning* is the process of finding the optimal values for the hyperparameters of a machine learning model. Hyperparameters are parameters that are not learned from the data but are set prior to training.\n",
        "    *   **Common Hyperparameters in Logistic Regression**:\n",
        "        *   `penalty`: Specifies the type of regularization to use (e.g., 'l1', 'l2', 'elasticnet').\n",
        "        *   `C`: Inverse of the regularization strength. Smaller values specify stronger regularization.\n",
        "        *   `solver`: Algorithm used for optimization (e.g., 'liblinear', 'lbfgs', 'sag', 'saga').\n",
        "        *   `l1_ratio`: Used when `penalty='elasticnet'`. Specifies the ratio of L1 penalty to L2 penalty.\n",
        "    *   **Techniques for Hyperparameter Tuning**:\n",
        "        1.  **Grid Search**: Exhaustively search through a predefined subset of the hyperparameter space.\n",
        "        2.  **Random Search**: Randomly sample hyperparameter combinations from a predefined distribution.\n",
        "        3.  **Cross-Validation**: Use cross-validation to evaluate the performance of each hyperparameter combination. Common techniques include k-fold cross-validation and stratified k-fold cross-validation.\n",
        "\n",
        "Question 14) What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "solution : In Logistic Regression, *solvers* are optimization algorithms used to find the parameters that minimize the cost function. Different solvers have different strengths and weaknesses and are suitable for different types of data and regularization.\n",
        "    *   **Common Solvers in Logistic Regression**:\n",
        "        1.  `liblinear`:\n",
        "\n",
        "            *   *Description*: A library for linear classification that supports L1 and L2 regularization.\n",
        "            *   *Suitable For*: Small to medium-sized datasets. Works well with L1 regularization.\n",
        "        2.  `lbfgs` (Limited-memory Broyden-Fletcher-Goldfarb-Shanno):\n",
        "\n",
        "            *   *Description*: A quasi-Newton method that approximates the Hessian matrix.\n",
        "            *   *Suitable For*: Small to medium-sized datasets. Does not support L1 regularization.\n",
        "        3.  `newton-cg`:\n",
        "\n",
        "            *   *Description*: A Newton-based method that uses the exact Hessian matrix.\n",
        "            *   *Suitable For*: Small to medium-sized datasets. Does not support L1 regularization.\n",
        "        4.  `sag` (Stochastic Average Gradient):\n",
        "\n",
        "            *   *Description*: An iterative method that uses a stochastic average gradient to update the parameters.\n",
        "            *   *Suitable For*: Large datasets. Supports L2 regularization.\n",
        "        5.  `saga` (Stochastic Average Gradient Algorithm):\n",
        "\n",
        "            *   *Description*: An extension of SAG that supports L1, L2, and Elastic Net regularization.\n",
        "            *   *Suitable For*: Large datasets. Works well with L1 regularization and Elastic Net.\n",
        "    *   **Which Solver to Use?**:\n",
        "\n",
        "        *   For small datasets: `liblinear` (especially with L1 regularization) or `lbfgs` (with L2 regularization).\n",
        "        *   For large datasets: `sag` (with L2 regularization) or `saga` (with L1, L2, or Elastic Net regularization).\n",
        "        *   If you need L1 regularization: `liblinear` (for small datasets) or `saga` (for large datasets).\n",
        "        *   If you need Elastic Net regularization: `saga`.\n",
        "\n",
        "Question :15) How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "solution : Logistic Regression is inherently a binary classification algorithm, but it can be extended to handle multiclass classification problems using two main approaches:\n",
        "        1.  **One-vs-Rest (OvR) / One-vs-All**:\n",
        "\n",
        "            *   *Description*: Train a separate Logistic Regression classifier for each class, treating it as the positive class and all other classes as the negative class.\n",
        "            *   *Prediction*: To classify a new instance, predict the probability of belonging to each class using the corresponding classifier. Assign the instance to the class with the highest probability.\n",
        "            *   *Advantages*: Simple to implement and can be used with any binary classification algorithm.\n",
        "            *   *Disadvantages*: Can suffer from class imbalance issues, as each classifier is trained on a different class distribution.\n",
        "        2.  **Softmax Regression (Multinomial Logistic Regression)**:\n",
        "\n",
        "            *   *Description*: Generalizes Logistic Regression to handle multiple classes directly. It uses the softmax function to compute the probability of belonging to each class.\n",
        "            *   *Prediction*: Assign the instance to the class with the highest probability.\n",
        "            *   *Advantages*: More efficient than OvR when the number of classes is large. Provides a more direct probability estimate for each class.\n",
        "            *   *Disadvantages*: Assumes that the classes are mutually exclusive.\n",
        "\n",
        "Question : 16) What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "solution :\n",
        "\n",
        "    *   **Advantages of Logistic Regression**:\n",
        "        1.  **Simple and Easy to Implement**: Logistic Regression is a straightforward algorithm that is easy to understand and implement.\n",
        "        2.  **Interpretable**: The coefficients of the model can be interpreted as the change in the log-odds of the outcome for a one-unit change in the predictor variable, making it easy to understand the impact of each feature on the outcome.\n",
        "        3.  **Efficient**: Logistic Regression is computationally efficient and can be trained quickly, even on large datasets.\n",
        "        4.  **Probabilistic Output**: Logistic Regression provides a probabilistic output, which can be useful for decision-making and risk assessment.\n",
        "        5.  **Well-understood Properties**: Logistic Regression has well-understood statistical properties and can be used to perform hypothesis testing and confidence interval estimation.\n",
        "    *   **Disadvantages of Logistic Regression**:\n",
        "        1.  **Linearity Assumption**: Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the outcome. This assumption may not hold in all cases, and the model may not perform well if the relationship is non-linear.\n",
        "        2.  **Sensitivity to Irrelevant Features**: Logistic Regression can be sensitive to irrelevant features, which can reduce the accuracy and interpretability of the model.\n",
        "        3.  **Limited Complexity**: Logistic Regression is a linear model and may not be able to capture complex relationships between the features and the outcome.\n",
        "        4.  **Multicollinearity**: Logistic Regression can be affected by multicollinearity among the independent variables, which can lead to unstable and unreliable coefficient estimates.\n",
        "        5.  **Class Imbalance**: Logistic Regression can be biased towards the majority class in imbalanced datasets, leading to poor performance on the minority class.\n",
        "\n",
        "Question : 17) What are some use cases of Logistic Regression?\n",
        "\n",
        "solution :\n",
        "    *   Logistic Regression is widely used in various fields for binary classification problems. Some common use cases include:\n",
        "        1.  **Spam Detection**: Identifying whether an email is spam or not spam based on features such as the content of the email, sender information, and subject line.\n",
        "        2.  **Medical Diagnosis**: Predicting whether a patient has a certain disease or condition based on symptoms, medical history, and test results.\n",
        "        3.  **Credit Risk Assessment**: Assessing the creditworthiness of loan applicants based on factors such as credit score, income, and employment history.\n",
        "        4.  **Customer Churn Prediction**: Predicting whether a customer will churn (stop using a service or product) based on factors such as usage patterns, customer demographics, and satisfaction scores.\n",
        "        5.  **Fraud Detection**: Identifying fraudulent transactions based on transaction history, user behavior, and other relevant features.\n",
        "        6.  **Political Campaigning**: Predicting whether a voter will vote for a particular candidate based on demographics, political views, and past voting behavior.\n",
        "        7.  **Marketing**: Predicting whether a customer will respond to a marketing campaign based on demographics, past purchase behavior, and other relevant factors.\n",
        "\n",
        "Question : 18) What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "solution :\n",
        "\n",
        "    *   The key difference between Softmax Regression and Logistic Regression lies in the number of classes they can handle:\n",
        "        *   **Logistic Regression**: Designed for binary classification problems, where the dependent variable has only two possible outcomes (e.g., 0 or 1, yes or no).\n",
        "        *   **Softmax Regression**: An extension of Logistic Regression that handles multiclass classification problems, where the dependent variable can have more than two possible outcomes (e.g., classifying images into different categories).\n",
        "    *   **Mathematical Difference**:\n",
        "        *   **Logistic Regression**: Uses the sigmoid function to model the probability of the positive class.\n",
        "        *   **Softmax Regression**: Uses the softmax function to model the probability of belonging to each class. The softmax function generalizes the sigmoid function to multiple classes.\n",
        "\n",
        "Question 19) How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "solution :  The choice between One-vs-Rest (OvR) and Softmax Regression for multiclass classification depends on the specific characteristics of the problem:\n",
        "        1.  **Mutually Exclusive Classes**:\n",
        "\n",
        "            *   **Softmax Regression**: Suitable when the classes are mutually exclusive, meaning that each instance can belong to only one class.\n",
        "            *   **OvR**: Can be used even when the classes are not mutually exclusive.\n",
        "        2.  **Number of Classes**:\n",
        "\n",
        "            *   **Softmax Regression**: More efficient than OvR when the number of classes is large, as it trains a single model instead of multiple models.\n",
        "            *   **OvR**: Can be more efficient than Softmax Regression when the number of classes is small.\n",
        "        3.  **Class Imbalance**:\n",
        "\n",
        "            *   **OvR**: Can suffer from class imbalance issues, as each classifier is trained on a different class distribution.\n",
        "            *   **Softmax Regression**: Can also be affected by class imbalance, but to a lesser extent than OvR.\n",
        "        4.  **Interpretability**:\n",
        "\n",
        "            *   **OvR**: Easier to interpret than Softmax Regression, as each classifier is trained to distinguish one class from all others.\n",
        "            *   **Softmax Regression**: Provides a more direct probability estimate for each class.\n",
        "    *   **General Guidelines**:\n",
        "        *   Use Softmax Regression when the classes are mutually exclusive and the number of classes is large.\n",
        "        *   Use OvR when the classes are not mutually exclusive or when you need to train multiple binary classifiers for each class.\n",
        "\n",
        "Question :20) How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "    *   In Logistic Regression, the coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, holding all other variables constant.\n",
        "        1.  **Log-Odds**: The log-odds (also known as the logit) is defined as `log(p / (1 - p))`, where `p` is the probability of the positive class.\n",
        "        2.  **Coefficient Interpretation**: A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the log-odds of the outcome, while a negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome.\n",
        "        3.  **Odds Ratio**: The odds ratio is the exponentiated value of the coefficient (e.g., `exp(coefficient)`). It represents the change in the odds of the outcome for a one-unit change in the predictor variable.\n",
        "            *   An odds ratio greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome.\n",
        "            *   An odds ratio less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome.\n",
        "            *   An odds ratio equal to 1 indicates that the predictor variable has no effect on the odds of the outcome.\n",
        "    *   **Example**:\n",
        "\n",
        "        *   Suppose the coefficient for a predictor variable \"age\" is 0.05. This means that for every one-year increase in age, the log-odds of the outcome increase by 0.05. The odds ratio is `exp(0.05) ≈ 1.0513`. This means that for every one-year increase in age, the odds of the outcome increase by approximately 5.13%.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JLk7bzIhZCfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRATICAL QUESTION :"
      ],
      "metadata": {
        "id": "pRZPVXz7ev58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJWRshDvYbUc",
        "outputId": "9e6c7b77-e50f-4c08-9ef6-9a8e0ee3235d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.85\n"
          ]
        }
      ],
      "source": [
        "##SOLUTION 1\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 2\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression_l1 = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
        "logistic_regression_l1.fit(X_train, y_train)\n",
        "y_pred_l1 = logistic_regression_l1.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
        "print(\"Model Accuracy with L1 Regularization:\", accuracy_l1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72wl71Mxh2Zj",
        "outputId": "cf890e1d-5254-4302-d23b-4c9c1b18d872"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 0.8466666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 3\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression_l2 = LogisticRegression(penalty='l2', random_state=42)\n",
        "logistic_regression_l2.fit(X_train, y_train)\n",
        "y_pred_l2 = logistic_regression_l2.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
        "print(\"Model Accuracy with L2 Regularization:\", accuracy_l2)\n",
        "print(\"Model Coefficients:\", logistic_regression_l2.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uig1BW1Difdn",
        "outputId": "b86563a5-828e-4f92-c335-24d1fb29d12e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 0.85\n",
            "Model Coefficients: [[ 0.04531971 -0.51367648  0.18043533  0.03922649  0.0191467   1.67543707\n",
            "  -0.05784131  0.00673649  0.00940495  0.04407345  0.15793238  0.4703682\n",
            "   0.04286485  0.16714098 -0.53771289  0.06245592  0.10203816 -0.066461\n",
            "  -0.79229387  0.0990962 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SOLUTION 4\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression_elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=42)\n",
        "logistic_regression_elasticnet.fit(X_train, y_train)\n",
        "y_pred_elasticnet = logistic_regression_elasticnet.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy_elasticnet = accuracy_score(y_test, y_pred_elasticnet)\n",
        "print(\"Model Accuracy with Elastic Net Regularization:\", accuracy_elasticnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmEqChoWit7O",
        "outputId": "9d29ef3a-abe4-40f9-c8f8-1de5b5b7a67f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 0.8533333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SOLUTION 5\n",
        "# 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42)\n",
        "logistic_regression_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = logistic_regression_ovr.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(\"Model Accuracy with OvR Multiclass Classification:\", accuracy_ovr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btUVNMMxi6nx",
        "outputId": "87452feb-f90a-446a-cf36-3ccc02e188b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with OvR Multiclass Classification: 0.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SOLUTION 6\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        " 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        " 'penalty': ['l1', 'l2', 'elasticnet']\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy with Best Model:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmCr0kaijDjl",
        "outputId": "cc5f22cb-4799-4878-ffca-61abf934abbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1'}\n",
            "Best Accuracy: 0.8814285714285713\n",
            "Test Accuracy with Best Model: 0.8466666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "30 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.50714286 0.87              nan 0.87428571 0.87714286        nan\n",
            " 0.88142857 0.86714286        nan 0.87142857 0.86857143        nan\n",
            " 0.87142857 0.87              nan 0.87       0.87              nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SOLUTION 7 :\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "\n",
        "n_splits = 5\n",
        "\n",
        "\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "cv_scores = cross_val_score(logistic_regression, X, y, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(cv_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXHg5WoUjS1j",
        "outputId": "4d11f54c-65c8-4b2a-87fa-534d4baca217"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 8\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np # Import numpy for mean calculation\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Car Sale.csv')\n",
        "\n",
        "# Identify numerical columns and fill missing values only in those columns\n",
        "# We use select_dtypes to pick only numeric columns\n",
        "numeric_cols = data.select_dtypes(include=np.number).columns\n",
        "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
        "\n",
        "# Assuming 'target_column' is the name of your target variable\n",
        "# Replace 'target_column' with the actual target column name if it's different\n",
        "# Check if the target column exists before dropping\n",
        "if 'target_column' in data.columns:\n",
        "    X = data.drop('target_column', axis=1)\n",
        "    y = data['target_column']\n",
        "\n",
        "    # Handle potential non-numeric columns in X after dropping the target\n",
        "    # Convert potentially object type columns to numeric, coercing errors to NaN\n",
        "    for col in X.columns:\n",
        "        if X[col].dtype == 'object':\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "    # Now fill any NaNs that might have been introduced by the coercion\n",
        "    X = X.fillna(X.mean(numeric_only=True))\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    logistic_regression = LogisticRegression(random_state=42)\n",
        "    logistic_regression.fit(X_train, y_train)\n",
        "    y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "else:\n",
        "    print(\"Error: 'target_column' not found in the dataset. Please check the column name.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp5pPlK2kby4",
        "outputId": "9cc94c92-ee3d-4a9a-bb8d-89778a064d74"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'target_column' not found in the dataset. Please check the column name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 9\n",
        "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from scipy.stats import uniform\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "param_distributions = {\n",
        " 'C': uniform(0.001, 100),\n",
        " 'penalty': ['l1', 'l2', 'elasticnet'],\n",
        " 'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "\n",
        "randomized_search = RandomizedSearchCV(LogisticRegression(random_state=42),\n",
        " param_distributions,\n",
        " n_iter=50,\n",
        " cv=5,\n",
        " scoring='accuracy',\n",
        " random_state=42)\n",
        "randomized_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Best Parameters:\", randomized_search.best_params_)\n",
        "print(\"Best Accuracy:\", randomized_search.best_score_)\n",
        "\n",
        "\n",
        "best_model = randomized_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy with Best Model:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na72zIVjkh_4",
        "outputId": "9c5646bc-3a3d-4f06-b2de-90f2738bcce9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(0.07887658410143283), 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.8842857142857141\n",
            "Test Accuracy with Best Model: 0.8433333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "70 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "40 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.87       0.87              nan 0.87142857        nan 0.86857143\n",
            " 0.88428571 0.87       0.87       0.87              nan 0.86857143\n",
            " 0.87              nan 0.87       0.87       0.87              nan\n",
            " 0.87142857 0.86857143 0.87       0.87              nan 0.87\n",
            " 0.87       0.87       0.87       0.87       0.87       0.87\n",
            " 0.87       0.87142857        nan        nan 0.87       0.87\n",
            " 0.87              nan        nan        nan 0.87       0.87\n",
            "        nan 0.87       0.87              nan 0.87       0.87\n",
            "        nan 0.87      ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "ovo_logistic_regression = OneVsOneClassifier(LogisticRegression(solver='liblinear', random_state=42))\n",
        "ovo_logistic_regression.fit(X_train, y_train)\n",
        "y_pred_ovo = ovo_logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(\"Model Accuracy with One-vs-One Multiclass Classification:\", accuracy_ovo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-zSu9IfkvXE",
        "outputId": "87a2ef93-96df-4b46-edbc-031ef72fd9f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-One Multiclass Classification: 0.7066666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 11\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "o0IBp9lclBhD",
        "outputId": "0d6aff6e-a8e6-4de8-8225-df6314b52f04"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 12\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQmsGj2lOr4",
        "outputId": "33dc0b0c-f407-48a7-91a6-74164df080be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7945205479452054\n",
            "Recall: 0.8285714285714286\n",
            "F1-Score: 0.8111888111888111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SOLUTION 13\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression_weighted = LogisticRegression(class_weight='balanced', random_state=42)\n",
        "logistic_regression_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = logistic_regression_weighted.predict(X_test)\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "print(\"Model Accuracy with Class Weights:\", accuracy_weighted)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy without Class Weights:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJwU3B_flh_7",
        "outputId": "7186a8cb-a50e-43ff-bc12-aa0c1ec41095"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Class Weights: 0.81\n",
            "Model Accuracy without Class Weights: 0.9233333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 14\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# Load the dataset - assuming 'titanic.csv' is in the same directory as the notebook\n",
        "# If the file is in a different location, provide the correct path.\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "\n",
        "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "X = data[features]\n",
        "y = data['Survived']\n",
        "\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X['Age'] = imputer.fit_transform(X[['Age']])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "MLxa2vgQmVPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##SOLUTION 15\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy without Scaling:\", accuracy)\n",
        "\n",
        "\n",
        "logistic_regression_scaled = LogisticRegression(random_state=42)\n",
        "logistic_regression_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logistic_regression_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Model Accuracy with Scaling:\", accuracy_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSti85FCmamB",
        "outputId": "279b2521-ccb9-476c-f7cf-7c88c8b2117e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy without Scaling: 0.85\n",
            "Model Accuracy with Scaling: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 16\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred_proba = logistic_regression.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQJDLApimpW9",
        "outputId": "daa2bfb9-2fc0-4fa6-818f-ab0a5f544740"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9071428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 17\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(C=0.5, random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with C=0.5:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t7PtVRrm5SE",
        "outputId": "edb49117-828b-4fa4-cbed-bd73db1ca718"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with C=0.5: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 18\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "feature_importance = np.abs(logistic_regression.coef_[0])\n",
        "important_features_indices = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "\n",
        "print(\"Important Feature Indices:\", important_features_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohgxUDcsnKfH",
        "outputId": "b10a442a-1f5d-486b-ed72-c0704a50274c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Feature Indices: [ 5 18 14  1 11  2 13 10 16 19 17 15  6  0  9 12  3  4  8  7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 19\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzaRquplnYdL",
        "outputId": "8d5e2fba-b5e9-4ed9-8e70-ff239e01e78e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.7002664298401421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 20\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_proba = logistic_regression.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "vzB5d4BLntf7",
        "outputId": "e451ec7c-1e07-49f4-d8c9-61e2af61fbca"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 21\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "\n",
        "for solver in solvers:\n",
        " logistic_regression = LogisticRegression(solver=solver, random_state=42)\n",
        " logistic_regression.fit(X_train, y_train)\n",
        " y_pred = logistic_regression.predict(X_test)\n",
        " accuracy = accuracy_score(y_test, y_pred)\n",
        " print(f\"Model Accuracy with solver={solver}:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVdgtQkvn79E",
        "outputId": "087a97ae-367f-4822-bd6b-0e7462655bfe"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with solver=liblinear: 0.85\n",
            "Model Accuracy with solver=saga: 0.85\n",
            "Model Accuracy with solver=lbfgs: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 22\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9thiiItoHlj",
        "outputId": "317363f0-0641-481d-af95-00ed30407ada"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.7015280729542706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 23\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "logistic_regression_raw = LogisticRegression(random_state=42)\n",
        "logistic_regression_raw.fit(X_train, y_train)\n",
        "y_pred_raw = logistic_regression_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Model Accuracy on Raw Data:\", accuracy_raw)\n",
        "\n",
        "\n",
        "logistic_regression_scaled = LogisticRegression(random_state=42)\n",
        "logistic_regression_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logistic_regression_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Model Accuracy on Standardized Data:\", accuracy_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd9YTTCMoUWn",
        "outputId": "a14d7b84-a460-4f00-e2d6-2235ec8b21cf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy on Raw Data: 0.85\n",
            "Model Accuracy on Standardized Data: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 24\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "best_C = None\n",
        "best_score = 0\n",
        "\n",
        "\n",
        "for C in C_values:\n",
        " logistic_regression = LogisticRegression(C=C, random_state=42)\n",
        " scores = cross_val_score(logistic_regression, X_train, y_train, cv=5, scoring='accuracy')\n",
        " mean_score = np.mean(scores)\n",
        " if mean_score > best_score:\n",
        "  best_score = mean_score\n",
        " best_C = C\n",
        "\n",
        "\n",
        "print(\"Optimal C:\", best_C)\n",
        "print(\"Best Cross-Validation Score:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD0HJmGBokzQ",
        "outputId": "76befc27-08ed-447a-fc48-086cc1f4f2bc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 100\n",
            "Best Cross-Validation Score: 0.877142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION 25\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "joblib.dump(logistic_regression, 'logistic_regression_model.joblib')\n",
        "\n",
        "\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy after loading:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVlI29kQo10m",
        "outputId": "f33f559f-07c6-4dd7-fc3a-1545e5722ec2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy after loading: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dUqY9xMVpHgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}